# ==== Dataset ==== #
dataset: Brennan2018
num_subjects: 49 # FIXME: this is not good if we want to subset subejects

# ==== Training ==== #
# ðŸ”¥ FIXME: with too large a batchsize there isn't enough 
# ðŸ”¥ FIXME: samples in the eval set, and the CLIPLoss produces nans silently
use_sampler: True # must be True to guarantee unique segments in a batch (a must for CLIP loss to work)
reproducible: False
num_workers: 8
batch_size: 64
updates: 1200
lr: 3e-4
lr_scheduler: multistep
lr_exp_gamma: 0.99
lr_step_numsteps: 5
lr_multistep_mlstns: [0.4, 0.6, 0.8, 0.9]
lr_step_gamma: 0.5
epochs: 3500
reduction: "mean"

# ==== Architecture ==== #
D1: 270
D2: 320
F: 512 # NOTE: because if you set last4layers=False, then it's set to 1024 in the dataset class
K: 32
d_drop: 0.3
# When calculating softmax for spatial attention in each iteration, we drop
# channels that are within the distance d_drop from a randomly selected channel

wav2vec_model: facebook/wav2vec2-large-xlsr-53 # (HuggingFace) # xlsr_53_56k (FAIR)

# == Hyperparameters that datasets should share === #
preprocs:
  audio_resample_rate: 16000 # before wav2vec
  lowpass_filter_width: 128

  brain_resample_rate: 120 # Hz
  brain_filter_low: 0.01 # Hz
  brain_filter_high: 35 # Hz

  seq_len_sec: 3        # segment length in seconds
  baseline_len_sec: 0.5 # baseline period in seconds
  preceding_chunk_for_baseline: False # False to calc baseline based on the current epoch
  shift_brain: True
  shift_len: 150 # ms

  last4layers: True # if True, the brain_encoder's emsize will be 1024, not 512
  mode: huggingface # (if last4layers: {'huggingface', 'before', 'after'}). If not huggingface, used FAIR's W2V impl.

  subject_wise: False
  clamp: True
  clamp_lim: 20
